---
title: Zero to Hero
description: My experience in following Andrej Karpathy's "Zero to Hero" YouTube course on neural networks.
---

### sept 3
Nevermind, I don't know anything about the multivariable case of the chain rule. So his easy fix for solving what to do when a node is referenced multiple times doesn't make that much sense to me (just add together the gradients? what? ... actually, are they just still called derivatives? AP Calc BC is not helping me out too much here ðŸ˜…)
Not a lot of progress since yesterday, but progress is progress.
lec 1: 1:31:42

### sept 2
so far, it's been pretty straightforward. Karpathy makes backpropagation seem so simple. I don't even think it's necessary to think about the chain rule per se in explaining how the gradient is a product of the derivatives (I think that's the proper terminology?), it seems almost intuitive that the way a weight influences the next weight has a multiplicative effect. Hmm. Now that I've written that out, I realize I have a ways to go in being able to *explain* the process to someone else.
lec 1, 1:15:08